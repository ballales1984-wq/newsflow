"""
AI Locale integrata per generare spiegazioni approfondite degli articoli
Usa modelli NLP pre-addestrati (T5, GPT-2, DistilBERT) completamente locali
Nessuna chiamata API esterna - tutto funziona offline!
"""
import os
import re
from typing import Optional, Dict
import warnings
warnings.filterwarnings('ignore')

# Import opzionali - installa solo se necessario
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ transformers non installato. Installa con: pip install transformers torch")

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ torch non installato. Installa con: pip install torch")

# Cache per modelli caricati (evita ricaricare ogni volta)
_model_cache = {}
_tokenizer_cache = {}


def _load_t5_model():
    """Carica modello T5-small per text-to-text generation"""
    if 't5' in _model_cache:
        return _model_cache['t5'], _tokenizer_cache['t5']
    
    if not TRANSFORMERS_AVAILABLE or not TORCH_AVAILABLE:
        return None, None
    
    try:
        print("ğŸ“¥ Caricamento modello T5-small (prima volta, puÃ² richiedere 1-2 minuti)...")
        model_name = "t5-small"  # Modello leggero (~240MB)
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
        # Metti in modalitÃ  eval per inferenza piÃ¹ veloce
        model.eval()
        
        # Cache
        _model_cache['t5'] = model
        _tokenizer_cache['t5'] = tokenizer
        
        print("âœ… Modello T5-small caricato!")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ Errore caricamento T5: {e}")
        return None, None


def _load_gpt2_model():
    """Carica modello GPT-2 per text generation"""
    if 'gpt2' in _model_cache:
        return _model_cache['gpt2'], _tokenizer_cache['gpt2']
    
    if not TRANSFORMERS_AVAILABLE or not TORCH_AVAILABLE:
        return None, None
    
    try:
        print("ğŸ“¥ Caricamento modello GPT-2 (prima volta, puÃ² richiedere 1-2 minuti)...")
        model_name = "gpt2"  # Modello leggero (~500MB)
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # Metti in modalitÃ  eval
        model.eval()
        
        # Cache
        _model_cache['gpt2'] = model
        _tokenizer_cache['gpt2'] = tokenizer
        
        print("âœ… Modello GPT-2 caricato!")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ Errore caricamento GPT-2: {e}")
        return None, None


def _generate_with_t5(prompt: str, max_length: int = 200) -> Optional[str]:
    """Genera testo usando T5-small"""
    model, tokenizer = _load_t5_model()
    if not model or not tokenizer:
        return None
    
    try:
        # T5 richiede prefisso per il task
        input_text = f"summarize: {prompt}"
        
        # Tokenizza
        inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
        
        # Genera
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=max_length,
                min_length=50,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2,
                temperature=0.7,
                do_sample=True
            )
        
        # Decodifica
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text.strip()
    except Exception as e:
        print(f"Errore generazione T5: {e}")
        return None


def _generate_with_gpt2(prompt: str, max_length: int = 200) -> Optional[str]:
    """Genera testo usando GPT-2"""
    model, tokenizer = _load_gpt2_model()
    if not model or not tokenizer:
        return None
    
    try:
        # Tokenizza
        inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=400, truncation=True)
        
        # Genera
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + max_length,
                min_length=inputs.shape[1] + 50,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Decodifica (rimuovi il prompt originale)
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Rimuovi il prompt originale dal risultato
        if prompt in generated_text:
            generated_text = generated_text.replace(prompt, "").strip()
        
        return generated_text.strip()
    except Exception as e:
        print(f"Errore generazione GPT-2: {e}")
        return None


def _create_structured_explanation(article: Dict, explanation_type: str, base_text: str) -> str:
    """Crea spiegazione strutturata usando il testo generato dall'AI"""
    
    title = article.get('title', '')
    summary = article.get('summary', '')
    keywords = article.get('keywords', [])
    author = article.get('author', 'Sconosciuto')
    quality_score = int((article.get('quality_score', 0.7) * 100))
    reading_time = article.get('reading_time_minutes', 3)
    
    if explanation_type == "quick":
        return f"""ğŸ¯ IN BREVE:

{title}

{base_text[:200] if base_text else summary[:200]}

ğŸ“ PERCHÃ‰ Ãˆ IMPORTANTE:
{base_text[200:400] if len(base_text) > 200 else 'Questa notizia tratta di ' + ', '.join(keywords[:3]) + ' ed Ã¨ rilevante per il settore.'}

â­ Quality Score: {quality_score}%
â±ï¸ Tempo lettura: {reading_time} minuti
ğŸ”— Fonte: {author}"""
    
    elif explanation_type == "standard":
        return f"""ğŸ“° CONTESTO:

{title}

{summary}

ğŸ” COSA SIGNIFICA:

{base_text[:300] if base_text else 'Questa notizia riguarda ' + ', '.join(keywords[:3]) + '.'}

ğŸ‘¥ CHI Ãˆ COINVOLTO:

â€¢ Autore: {author}
â€¢ Fonte: {article.get('url', '').split('/')[2] if article.get('url') else 'Non disponibile'}
â€¢ Categoria: {keywords[0] if keywords else 'Generale'}
â€¢ Lingua: {article.get('language', 'it').upper()}

ğŸ“Š ANALISI QUALITÃ€:

â€¢ Quality Score: {quality_score}% - {'Eccellente' if quality_score >= 85 else 'Molto buona' if quality_score >= 75 else 'Buona'}
â€¢ Tempo lettura: {reading_time} minuti
â€¢ Verificato: {'SÃ¬ âœ“' if article.get('is_verified') else 'In revisione'}

ğŸ“ PAROLE CHIAVE:
{', '.join(keywords) if keywords else 'Nessuna'}

ğŸŒ IMPATTO:

{base_text[300:600] if len(base_text) > 300 else 'Questa notizia Ã¨ stata selezionata perchÃ© supera i nostri standard di qualitÃ  e tratta temi rilevanti.'}

ğŸ”— PER APPROFONDIRE:
Leggi l'articolo completo su: {article.get('url', '')}"""
    
    else:  # deep
        content = article.get('content', '')[:800] if article.get('content') else summary[:800]
        
        return f"""ğŸ“š ANALISI APPROFONDITA:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{title}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ SINTESI COMPLETA:

{summary}

{content + '...' if content else ''}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ§  ANALISI AI GENERATA:

{base_text if base_text else 'Analisi in corso...'}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‘¥ ATTORI E STAKEHOLDER:

â€¢ Autore/Fonte: {author}
â€¢ Piattaforma: {article.get('url', '').split('/')[2] if article.get('url') else 'Non disponibile'}
â€¢ Target audience: Lettori interessati a {keywords[0] if keywords else 'informazione'}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸŒ CONSEGUENZE E IMPLICAZIONI:

Le implicazioni di questa notizia potrebbero influenzare:
â€¢ Policy makers e regolatori (nuove normative)
â€¢ Aziende del settore (strategie e investimenti)
â€¢ Professionisti e sviluppatori (competenze richieste)
â€¢ Utenti finali e cittadini (impatto quotidiano)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š METRICHE DI QUALITÃ€:

â€¢ Quality Score: {quality_score}%
â€¢ Tempo lettura: {reading_time} minuti
â€¢ Lingua: {article.get('language', 'it').upper()}
â€¢ Status: {'Verificato âœ“' if article.get('is_verified') else 'In revisione'}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ PERCHÃ‰ LEGGERE QUESTO ARTICOLO:

Questa notizia Ã¨ stata curata e selezionata dal nostro sistema perchÃ© rappresenta 
contenuto di alta qualitÃ  ({quality_score}%) su temi di {keywords[0] if keywords else 'informazione'} 
rilevanti per i lettori di NewsFlow."""


def generate_explanation_local_ai(article: Dict, explanation_type: str = "quick") -> str:
    """
    Genera spiegazione usando AI locale integrata (T5 o GPT-2)
    
    Args:
        article: Dizionario con dati dell'articolo
        explanation_type: "quick" (30s), "standard" (3min), "deep" (approfondito)
    
    Returns:
        Spiegazione generata con AI locale
    """
    if not TRANSFORMERS_AVAILABLE or not TORCH_AVAILABLE:
        return _generate_fallback_explanation(article, explanation_type)
    
    # Prepara prompt in base al tipo
    title = article.get('title', '')
    summary = article.get('summary', '')[:800]
    keywords = ', '.join(article.get('keywords', [])[:5])
    content = article.get('content', '')[:1000] if article.get('content') else summary[:1000]
    
    if explanation_type == "quick":
        prompt = f"""Spiega brevemente questa notizia in italiano:

Titolo: {title}
Riassunto: {summary[:400]}

Cosa Ã¨ successo e perchÃ© Ã¨ importante?"""
        max_length = 150
    
    elif explanation_type == "standard":
        prompt = f"""Spiega in dettaglio questa notizia in italiano:

Titolo: {title}
Riassunto: {summary}
Parole chiave: {keywords}

Fornisci contesto, chi Ã¨ coinvolto e l'impatto."""
        max_length = 300
    
    else:  # deep
        prompt = f"""Fornisci un'analisi approfondita di questa notizia in italiano:

Titolo: {title}
Riassunto: {summary}
Contenuto: {content}
Parole chiave: {keywords}

Analizza il contesto storico, gli attori coinvolti, le conseguenze e le prospettive future."""
        max_length = 500
    
    # Prova prima T5 (migliore per summarization)
    generated_text = _generate_with_t5(prompt, max_length)
    
    # Fallback a GPT-2 se T5 non funziona
    if not generated_text or len(generated_text) < 50:
        print("âš ï¸ T5 non ha generato testo sufficiente, provo GPT-2...")
        generated_text = _generate_with_gpt2(prompt, max_length)
    
    # Se ancora nulla, usa fallback statico
    if not generated_text or len(generated_text) < 50:
        print("âš ï¸ AI locale non disponibile, uso spiegazione statica")
        return _generate_fallback_explanation(article, explanation_type)
    
    # Crea spiegazione strutturata
    return _create_structured_explanation(article, explanation_type, generated_text)


def _generate_fallback_explanation(article: Dict, explanation_type: str) -> str:
    """Fallback a spiegazione statica se AI non disponibile"""
    title = article.get('title', '')
    summary = article.get('summary', '')
    keywords = article.get('keywords', [])
    author = article.get('author', 'Sconosciuto')
    quality_score = int((article.get('quality_score', 0.7) * 100))
    reading_time = article.get('reading_time_minutes', 3)
    
    if explanation_type == "quick":
        return f"""ğŸ¯ IN BREVE:

{title}

{summary[:250]}...

ğŸ“ PERCHÃ‰ Ãˆ IMPORTANTE:
Questa notizia tratta di {', '.join(keywords[:3]) if keywords else 'informazione'} ed Ã¨ rilevante.

â­ Quality Score: {quality_score}%
â±ï¸ Tempo lettura: {reading_time} minuti
ğŸ”— Fonte: {author}"""
    
    elif explanation_type == "standard":
        return f"""ğŸ“° CONTESTO:

{title}

{summary}

ğŸ” COSA SIGNIFICA:

Questa notizia riguarda {', '.join(keywords[:3]) if keywords else 'informazione'}.

ğŸ‘¥ CHI Ãˆ COINVOLTO:
â€¢ Autore: {author}
â€¢ Categoria: {keywords[0] if keywords else 'Generale'}

ğŸ“Š ANALISI QUALITÃ€:
â€¢ Quality Score: {quality_score}%
â€¢ Tempo lettura: {reading_time} minuti"""
    
    else:  # deep
        return f"""ğŸ“š ANALISI APPROFONDITA:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{title}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ SINTESI COMPLETA:

{summary}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ PERCHÃ‰ LEGGERE QUESTO ARTICOLO:

Questa notizia Ã¨ stata curata perchÃ© rappresenta contenuto di alta qualitÃ  ({quality_score}%) 
su temi di {keywords[0] if keywords else 'informazione'} rilevanti per i lettori di NewsFlow."""

